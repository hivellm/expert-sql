# Expert SQL

SQL query generation expert trained on high-quality synthetic SQL dataset with validated PostgreSQL syntax.

**Version:** 0.2.0 | **Checkpoint:** 1250 | **Quality Score:** 9.6/10 | **Real-world Success:** 100% (30/30)

## Quick Start

```bash
# 1. Download package
wget https://github.com/hivellm/expert-sql/releases/download/v0.2.0/expert-sql-qwen3-0-6b.v0.2.0.expert

# 2. Install
expert-cli install expert-sql-qwen3-0-6b.v0.2.0.expert

# 3. Use
expert-cli chat --experts sql
> List all users who registered in the last month
```

**Works best for:** E-commerce, CRM, Analytics, Business Reports  
**Limitations:** No recursive CTEs, no UNION operations (see Known Limitations below)

## Features

- ‚úÖ **PostgreSQL dialect** with MySQL‚ÜíPostgreSQL syntax conversion
- ‚úÖ **Schema-aware** query generation with ChatML format
- ‚úÖ **DoRA adapter (r=12)** for complex queries (JOINs, subqueries, window functions)
- ‚úÖ **Grammar validation** (GBNF) for syntax enforcement
- ‚úÖ **Unsloth integration** - 2x faster training, 70% less VRAM
- ‚úÖ **SQL validation** via sqlglot for dataset quality
- ‚úÖ **Windows optimized** with memory safety and CUDA support
- ‚úÖ **99,935 validated examples** from gretelai/synthetic_text_to_sql
- ‚úÖ **Production-ready** - Checkpoint 1250 (9.6/10 quality, 100% real-world success)

## What It Can Do ‚úÖ

**Excellent Support (95-100% success rate):**
- ‚úÖ SELECT queries with WHERE, ORDER BY, LIMIT
- ‚úÖ INNER JOIN and multi-table joins (2-4 tables)
- ‚úÖ Aggregations (COUNT, SUM, AVG, MIN, MAX, GROUP BY, HAVING)
- ‚úÖ Subqueries (WHERE IN, EXISTS, NOT EXISTS)
- ‚úÖ Date operations (EXTRACT, BETWEEN, INTERVAL)
- ‚úÖ String operations (LIKE, CONCAT, UPPER/LOWER)
- ‚úÖ Window functions (ROW_NUMBER, RANK, PARTITION BY)
- ‚úÖ Common CTEs (WITH clause, non-recursive)
- ‚úÖ Filtering (IN, BETWEEN, multiple conditions)
- ‚úÖ Practical business queries (e-commerce, CRM, analytics, reports)

**Example Queries Generated:**

```sql
-- E-commerce: Products with low stock
SELECT name, price, stock 
FROM products 
WHERE stock < min_stock 
ORDER BY stock ASC;

-- CRM: Customers without orders (uses NOT EXISTS!)
SELECT c.name 
FROM customers c 
WHERE NOT EXISTS (
  SELECT o.customer_id FROM orders o 
  WHERE o.customer_id = c.id
);

-- Analytics: Top 10 customers by revenue
SELECT c.name, SUM(o.total) as total_spent 
FROM orders o 
JOIN customers c ON o.customer_id = c.id 
GROUP BY c.name 
ORDER BY total_spent DESC 
LIMIT 10;

-- Window Function: Sales ranking by region
SELECT salesperson, region, 
       ROW_NUMBER() OVER (PARTITION BY region ORDER BY SUM(amount) DESC) as rank
FROM sales 
GROUP BY salesperson, region;
```

## Known Limitations ‚ö†Ô∏è

**These patterns have lower success rates or are not supported:**
- ‚ùå **Recursive CTEs** (WITH RECURSIVE) - generates self-join instead
- ‚ùå **UNION/UNION ALL** - incorrectly uses JOIN with OR
- ‚ùå **LEFT JOIN with NULL checks** - prefers INNER JOIN
- ‚ö†Ô∏è **Complex percentage calculations** - gets division logic wrong
- ‚ö†Ô∏è **Deeply nested CASE WHEN** (3+ levels) - only generates simple cases
- ‚ö†Ô∏è **Column alias consistency** - occasional ORDER BY alias errors

**Recommendation:** Use for 95% of typical web application queries. For recursive hierarchies or complex set operations, validate and adjust the generated SQL manually.

## Training

### Quick Start

```bash
# From expert-sql directory
cd F:/Node/hivellm/expert/experts/expert-sql

# Run training (uses HuggingFace dataset directly)
../../cli/target/release/expert-cli train
```

### Dataset Preprocessing (ALREADY DONE)

**Current Dataset**: gretelai/synthetic_text_to_sql (99,935 examples)
- ‚úÖ Pre-processed with MySQL‚ÜíPostgreSQL conversion
- ‚úÖ Validated with sqlglot
- ‚úÖ Deduplicated by question
- ‚úÖ ChatML formatted
- ‚úÖ Optimized (text-only, 77% size reduction)

**Location**: `datasets/processed/train.jsonl`

### Custom Dataset Preprocessing (Optional)

To preprocess a different dataset:

```bash
# Install dependencies
pip install datasets sqlglot

# Run preprocessing with SQL validation
python preprocess.py \
  --dataset your-dataset-name \
  --dialect postgres \
  --output datasets/custom \
  --validate-sql

# Update manifest.json to use your dataset
```

### Preprocessing Features

```bash
python preprocess.py --help

Key Features:
  --dataset           HuggingFace dataset (default: gretelai/synthetic_text_to_sql)
  --output            Output directory (default: datasets/processed)
  --dialect           SQL dialect: postgres/mysql/sqlite (default: postgres)
  --validate-sql      Enable SQL validation and MySQL‚ÜíPostgreSQL conversion
  --no-deduplicate    Skip deduplication
  --format            chatml or simple (default: chatml for Qwen3)
```

### What Preprocessing Does

1. **SQL Validation & Conversion**:
   - Validates SQL syntax with sqlglot
   - Converts MySQL syntax to PostgreSQL
   - Fixes `YEAR()`, `MONTH()`, `STR_TO_DATE()` ‚Üí `EXTRACT()`
   - Removes invalid SQL examples

2. **Schema Canonicalization**:
   - Normalizes whitespace
   - Formats CREATE TABLE statements consistently
   - Preserves SQL case sensitivity

3. **ChatML Formatting** (Qwen3 native):
   ```
   <|system|>
   Dialect: postgres
   Schema:
   CREATE TABLE users (id INT, name VARCHAR(100))
   <|end|>
   <|user|>
   Show all users
   <|end|>
   <|assistant|>
   SELECT * FROM users;
   <|end|>
   ```

4. **Quality Filtering**:
   - Removes duplicates by question (exact match)
   - Validates schema presence
   - Removes invalid SQL (syntax errors)
   - Optimizes to text-only format (77% size reduction)

## Configuration

### Adapter: DoRA r=12

```json
{
  "type": "dora",
  "rank": 12,
  "alpha": 24,
  "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "up_proj", "down_proj"]
}
```

- **Why DoRA?** Better quality than LoRA for complex queries (JOINs, subqueries)
- **Why r=12?** Balanced capacity for SQL (8-16 range)
- **Why full modules?** MLP (up/down) crucial for SQL patterns

### Decoding: Optimized (Unsloth/Qwen Recommended)

```json
{
  "temperature": 0.7,
  "top_p": 0.8,
  "top_k": 20,
  "grammar_type": "sql-postgres",
  "validation": "parser-strict",
  "stop_sequences": [";", "\n\n"]
}
```

- **Temperature 0.7**: Qwen3 recommended (prevents repetition collapse)
- **Top-P 0.8**: Unsloth recommended (better diversity)
- **Top-K 20**: Focused sampling (from 50)
- **Grammar**: Enforces valid SQL syntax
- **Stop sequences**: Prevents over-generation

### Training: Windows Optimized + Unsloth

```json
{
  "use_unsloth": true,
  "batch_size": 2,
  "gradient_accumulation_steps": 45,
  "learning_rate": 5e-5,
  "warmup_ratio": 0.1,
  "dropout": 0.1,
  "epochs": 1.5,
  "lr_scheduler": "cosine",
  "use_sdpa": true,
  "bf16": true,
  "torch_compile": false
}
```

**Performance**:
- **Effective batch**: 90 (2 √ó 45) - compensates for small batch
- **VRAM usage**: ~0.56GB / 24GB (2.3%) - 70% reduction with Unsloth
- **Training speed**: ~2x faster with Unsloth vs standard PyTorch
- **Windows safe**: Small batch prevents memory issues

**Optimizations**:
- ‚úÖ **Unsloth**: 2x faster, 70% less VRAM
- ‚úÖ **Low LR (5e-5)**: LLaMA-Factory best practice for small models
- ‚úÖ **Warmup 10%**: Scales with dataset size
- ‚úÖ **Higher dropout (0.1)**: Better generalization
- ‚úÖ **Cosine LR**: Conservative decay (no restarts)
- ‚úÖ **torch_compile disabled**: Windows compatibility (Triton issue)

## Performance

### Actual Results (Checkpoint-1250)

**Quality Score**: 9.6/10 (Real-world queries benchmark)

- ‚úÖ **SQL Generation**: 100% success rate (30/30 real-world test cases)
- ‚úÖ **Syntax Correctness**: 100% (all queries valid PostgreSQL)
- ‚úÖ **Practical Queries**: 95% production-ready
- ‚úÖ **Training Efficiency**: Optimal convergence at checkpoint-1250
- ‚úÖ **VRAM Usage**: 0.56GB during training (70% reduction with Unsloth)
- ‚úÖ **Inference Speed**: ~100-150ms per query (RTX 4090)

### Real-World Query Test (30 Scenarios)

**Test Suite Coverage:**

| Category | Scenarios | Success | Examples |
|----------|-----------|---------|----------|
| üõí E-commerce | 5/5 (100%) | ‚úÖ | Products in stock, total sales, top sellers, revenue by category |
| üë• CRM | 4/4 (100%) | ‚úÖ | Customers without orders, total spent, VIP customers, new signups |
| üìà Analytics | 4/4 (100%) | ‚úÖ | Sales by day, conversion rate, average ticket, inactive products |
| üîç Filters | 4/4 (100%) | ‚úÖ | Name search, multiple values, price range, date filtering |
| üìÑ Reports | 3/3 (100%) | ‚úÖ | Top customers, low stock alerts, pending orders |
| üîó Joins | 3/3 (100%) | ‚úÖ | Customer orders, product categories, user last order |
| ‚ûï Aggregations | 3/3 (100%) | ‚úÖ | Count by status, average price, min/max by group |
| üõ†Ô∏è Practical | 4/4 (100%) | ‚úÖ | Duplicates, unique emails, stock updates, inactive users |

**By Difficulty:**
- **B√°sico** (17/17): Simple SELECT, WHERE, ORDER BY, basic JOINs
- **Intermedi√°rio** (13/13): Multi-table JOINs, subqueries, aggregations, window functions

**Overall:** 30/30 scenarios passed (100% success rate)

### Checkpoint Evolution

| Checkpoint | Epoch | Quality Score | SQL Valid | Real-World Test | Notes |
|------------|-------|---------------|-----------|-----------------|-------|
| Base Model | 0.0   | 0.0/10        | 0/30 (0%) | N/A | Only explanations, no SQL |
| Checkpoint-750 | 0.75 | 8.5/10     | 30/30 (100%) | Good | Solid performance |
| Checkpoint-1000 | 1.0 | 9.0/10     | 30/30 (100%) | Better | Improved joins |
| Checkpoint-1250 | 1.25 | 9.6/10     | 30/30 (100%) | **Best** | **Production-ready** ‚≠ê |
| Checkpoint-1500 | 1.5 | 9.2/10     | 30/30 (100%) | Slight degradation | Overfitting signs |

**Conclusion**: Checkpoint-1250 is optimal for production use.

### Qualitative Analysis (Complex Scenarios)

**Checkpoint-1250 Performance** on 10 advanced SQL scenarios:

| Scenario | CKP-1250 | Status | Notes |
|----------|----------|--------|-------|
| Multiple JOIN + Aggregation | 8/10 | ‚úÖ Good | Correct JOINs, proper GROUP BY |
| Correlated Subquery | 10/10 | ‚úÖ Excellent | Perfect NOT EXISTS usage |
| Window Function | 9/10 | ‚úÖ Excellent | ROW_NUMBER + PARTITION correct |
| Recursive CTE | 2/10 | ‚ùå Weak | Uses self-join instead of WITH RECURSIVE |
| UNION + Aggregations | 3/10 | ‚ùå Weak | Uses JOIN instead of UNION |
| Subquery in SELECT/WHERE | 5/10 | ‚ö†Ô∏è Fair | Correct structure, wrong calculation |
| Complex HAVING | 9/10 | ‚úÖ Excellent | COUNT(*) + HAVING perfect |
| Multiple LEFT JOIN | 6/10 | ‚ö†Ô∏è Fair | Uses INNER JOIN, misses NULL checks |
| Nested CASE WHEN | 5/10 | ‚ö†Ô∏è Fair | Simple CASE, doesn't nest deeply |
| EXISTS vs IN | 10/10 | ‚úÖ Excellent | Optimal query structure |
| **AVERAGE** | **6.7/10** | | Significant improvement over earlier checkpoints |

### Strengths & Limitations

**Strengths** ‚úÖ:
- ‚úÖ **Perfect on practical queries**: 100% (30/30) real-world scenarios
- ‚úÖ **Excellent JOINs**: INNER JOIN, multi-table joins work perfectly
- ‚úÖ **Strong aggregations**: SUM, COUNT, AVG, GROUP BY, HAVING
- ‚úÖ **Subqueries**: NOT EXISTS, IN, correlated subqueries
- ‚úÖ **Date handling**: EXTRACT, BETWEEN, INTERVAL
- ‚úÖ **Filters**: WHERE, LIKE, IN, multiple conditions
- ‚úÖ **Window functions**: ROW_NUMBER(), PARTITION BY
- ‚úÖ **Clean output**: Concise SQL, no over-explanation

**Known Limitations** ‚ö†Ô∏è:
- ‚ùå **Recursive CTEs**: Cannot generate `WITH RECURSIVE` - uses self-join instead
- ‚ùå **UNION queries**: Incorrectly uses JOIN with OR instead of UNION/UNION ALL
- ‚ùå **LEFT JOIN with NULL**: Prefers INNER JOIN, doesn't check IS NULL correctly
- ‚ö†Ô∏è **Complex percentages**: Gets calculation logic wrong (divides by itself)
- ‚ö†Ô∏è **Nested CASE WHEN**: Only generates simple 2-level CASE, not deeply nested
- ‚ö†Ô∏è **Column aliases**: Occasional mismatch between alias definition and usage in ORDER BY

**Production Readiness** üéØ:
- ‚úÖ Use for: E-commerce, CRM, Analytics, Reports, Dashboards (95% of real use cases)
- ‚ö†Ô∏è Avoid for: Data warehousing with recursive hierarchies, complex UNION operations
- ‚úÖ Safe for: Web applications, REST APIs, admin panels, business reports

**Recommended**: Use Checkpoint-1250 for production (9.6/10 quality score, 100% success on practical queries)

## Installation & Usage

### From Package (.expert file)

```bash
# 1. Download or build the package
expert-cli package --manifest manifest.json --weights weights

# 2. Install the expert
expert-cli install expert-sql-qwen3-0-6b.v0.2.0.expert

# 3. Use in chat
expert-cli chat --experts sql
```

### Package Structure (v0.2.0)

**Package Naming:**
```
expert-sql-qwen3-0-6b.v0.2.0.expert
‚îÇ       ‚îÇ       ‚îÇ         ‚îÇ      ‚îî‚îÄ Extension (.expert = HiveLLM expert package)
‚îÇ       ‚îÇ       ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Version (semver: 0.2.0)
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Base model (Qwen3-0.6B normalized)
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Expert name
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Prefix
```

The `.expert` package contains all files in the **root** (no subdirectories):

```
expert-sql-qwen3-0-6b.v0.2.0.expert (tar.gz, 25.9 MB)
‚îú‚îÄ‚îÄ manifest.json                 # Expert metadata and configuration
‚îú‚îÄ‚îÄ adapter_config.json           # PEFT adapter configuration (DoRA)
‚îú‚îÄ‚îÄ adapter_model.safetensors     # DoRA adapter weights (25.8 MB)
‚îú‚îÄ‚îÄ tokenizer.json                # Tokenizer vocabulary (11.4 MB)
‚îú‚îÄ‚îÄ tokenizer_config.json         # Tokenizer configuration
‚îú‚îÄ‚îÄ special_tokens_map.json       # Special tokens mapping
‚îú‚îÄ‚îÄ training_args.bin             # Training hyperparameters
‚îú‚îÄ‚îÄ vocab.json                    # Vocabulary mappings (2.8 MB)
‚îú‚îÄ‚îÄ README.md                     # Documentation
‚îú‚îÄ‚îÄ grammar.gbnf                  # PostgreSQL GBNF grammar
‚îî‚îÄ‚îÄ LICENSE                       # CC-BY-4.0 license
```

**Key Features:**
- ‚úÖ All files in root (no nested directories) - easier loading
- ‚úÖ Checkpoint-1250 included (best performance, not final)
- ‚úÖ SHA256 checksum included for integrity verification
- ‚úÖ Compatible with expert-cli v0.2.3+
- ‚úÖ Cross-platform (Linux, macOS, Windows with tar support)

### Building the Package

```bash
# From expert-sql directory
cd F:/Node/hivellm/expert/experts/expert-sql

# Create package (uses checkpoint-1250 from manifest)
../../cli/target/release/expert-cli package --manifest manifest.json --weights weights

# Output: expert-sql-qwen3-0-6b.v0.2.0.expert (25.9 MB)
# Checksum: expert-sql-qwen3-0-6b.v0.2.0.expert.sha256
```

**How Checkpoint Selection Works:**

The manifest contains `packaging_checkpoint: "checkpoint-1250"` which tells the packaging system to use checkpoint-1250 instead of the final checkpoint. This is crucial when the best model is not the last trained checkpoint.

```json
{
  "training": {
    "packaging_checkpoint": "checkpoint-1250"
  }
}
```

The system will:
1. Read the `packaging_checkpoint` field
2. Adjust adapter paths from `final` ‚Üí `checkpoint-1250`
3. Extract files from `weights/qwen3-06b/checkpoint-1250/`
4. Place all files in the **root** of the package
5. Generate SHA256 checksum

### Testing the Package

```powershell
# Test packaged expert inference
.\test_packaged_expert.ps1

# This will:
# 1. Extract the package
# 2. Validate structure (11 files in root)
# 3. Run 3 SQL generation tests
# 4. Clean up
```

Expected output: **3/3 queries generated successfully**

### Verifying Package Integrity

```bash
# Verify SHA256 checksum
sha256sum -c expert-sql-qwen3-0-6b.v0.2.0.expert.sha256

# Expected output:
# expert-sql-qwen3-0-6b.v0.2.0.expert: OK
```

**Package Info:**
- **Size**: 25.9 MB (compressed)
- **Format**: tar.gz
- **Checksum**: Included in `.sha256` file
- **Extraction**: Standard tar/gzip tools
- **Compatibility**: Linux, macOS, Windows (with tar support)

## Testing

### Packaged Inference Test

**Quick validation** that the packaged expert works correctly:

```powershell
# Test inference from .expert package
.\test_packaged_expert.ps1
```

Expected output: 3/3 SQL queries generated successfully.

### Comparison Tests (Base vs Expert)

Validate that the expert outperforms the base model:

```powershell
# Run automated comparison tests
.\test.ps1 -TestSuite comparison

# Run interactive comparison with custom examples
.\run_interactive_comparison.ps1
```

**Test Results**: See test output for detailed analysis.

**Key Findings** (v0.2.0):
- ‚úÖ **100% success rate** - 30/30 real-world scenarios
- ‚úÖ **Perfect JOINs**: Multi-table joins work flawlessly
- ‚úÖ **Strong aggregations**: GROUP BY, HAVING, window functions
- ‚úÖ **Production-ready**: Safe for web apps, APIs, dashboards

### Available Test Suites

```powershell
# Quick test - Basic comparison only
.\test.ps1 -TestSuite comparison

# Comprehensive test - 50+ SQL patterns
.\test.ps1 -TestSuite comprehensive

# Advanced test - Window functions, CTEs, complex patterns
.\test.ps1 -TestSuite advanced

# Run ALL test suites with detailed reporting
.\run_all_tests.ps1

# Quick mode (comparison only)
.\run_all_tests.ps1 -QuickTest

# Skip advanced tests (faster)
.\run_all_tests.ps1 -SkipAdvanced
```

### Test Coverage

The test suites cover **100+ different SQL scenarios**:

**Basic Patterns** (test_comparison.py):
- ‚úì Simple SELECT with WHERE
- ‚úì COUNT aggregations
- ‚úì JOIN operations
- ‚úì GROUP BY aggregations
- ‚úì BETWEEN filtering

**Comprehensive Patterns** (test_comprehensive.py):
- ‚úì Subqueries (WHERE, FROM, SELECT)
- ‚úì Multiple JOINs (3+ tables)
- ‚úì LEFT/RIGHT/OUTER JOINs
- ‚úì Multiple aggregations (AVG, SUM, COUNT, MIN, MAX)
- ‚úì HAVING clause
- ‚úì ORDER BY + LIMIT + OFFSET
- ‚úì DISTINCT and COUNT(DISTINCT)
- ‚úì CASE WHEN statements
- ‚úì NULL handling (IS NULL, IS NOT NULL, COALESCE)
- ‚úì Date filtering
- ‚úì String functions (LIKE, concatenation)
- ‚úì Complex filtering (IN, AND/OR combinations)

**Advanced Patterns** (test_advanced.py):
- ‚úì Window functions (ROW_NUMBER, RANK, DENSE_RANK)
- ‚úì PARTITION BY
- ‚úì Running totals (SUM OVER)
- ‚úì Common Table Expressions (CTEs with WITH)
- ‚úì Multiple CTEs
- ‚úì Set operations (UNION, UNION ALL, INTERSECT)
- ‚úì Self-joins
- ‚úì GROUP BY multiple columns
- ‚úì Conditional aggregations
- ‚úì Correlated subqueries (EXISTS, NOT EXISTS)
- ‚úì Mathematical operations
- ‚úì Top-N per group queries

### Performance Benchmarking

```powershell
# Benchmark inference speed and memory usage
.\benchmark_performance.ps1

# Run 50 iterations for more accurate results
.\benchmark_performance.ps1 -Iterations 50

# Show detailed output for each iteration
.\benchmark_performance.ps1 -DetailedOutput
```

## Usage

### Interactive Chat

```bash
# Start interactive SQL generation
expert-cli chat --experts sql

# Example queries:
> List all users who registered in the last 30 days
> Show top 10 products by revenue with category
> Find customers who never made an order
```

### Single Query Mode

```bash
# Generate single SQL query
expert-cli chat --experts sql --prompt "List all users who made orders in 2024"
```

### Python Integration

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Load base model
base_model_path = "F:/Node/hivellm/expert/models/Qwen3-0.6B"
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)

# Load expert adapter (from extracted package or checkpoint)
adapter_path = "experts/expert-sql"  # or path to extracted .expert
model = PeftModel.from_pretrained(base_model, adapter_path)

# Generate SQL
schema = "CREATE TABLE users (id INT, name VARCHAR, email VARCHAR, created_at DATE)"
question = "List users registered in 2024"

messages = [
    {"role": "system", "content": f"Dialect: postgres\nSchema:\n{schema}"},
    {"role": "user", "content": question}
]

text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer([text], return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=150,
        temperature=0.7,
        top_p=0.8,
        top_k=20
    )

sql = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
print(sql)
# Output: SELECT * FROM users WHERE EXTRACT(YEAR FROM created_at) = 2024;
```

## Dataset

### Source: gretelai/synthetic_text_to_sql

- **Examples**: 99,935 validated training examples (from 100k original)
- **Tasks**: Text-to-SQL with schema context
- **Languages**: English + Portuguese questions
- **Dialect**: PostgreSQL (converted from MySQL)
- **Quality**: Validated with sqlglot, syntax errors removed (99.93% valid)

### Why This Dataset?

‚úÖ **Higher Quality**: Synthetic generation with validation  
‚úÖ **Better Coverage**: Diverse SQL patterns (SELECT, JOIN, subqueries, aggregations, window functions)  
‚úÖ **Clean Syntax**: MySQL‚ÜíPostgreSQL conversion applied automatically  
‚úÖ **Optimized Size**: Text-only format (77% smaller than original)  
‚úÖ **Real-world patterns**: Covers e-commerce, CRM, analytics use cases

**Preprocessing Applied**:
- MySQL functions converted to PostgreSQL (`YEAR()` ‚Üí `EXTRACT(YEAR FROM ...)`)
- Invalid SQL removed via sqlglot validation
- Deduplicated by question (exact matches removed)
- ChatML formatted for Qwen3 native support

### Dataset Structure (After Preprocessing)

```json
{
  "text": "<|system|>\nDialect: postgres\nSchema:\nCREATE TABLE users (id INT, name VARCHAR)...\n<|end|>\n<|user|>\nShow all users\n<|end|>\n<|assistant|>\nSELECT * FROM users;\n<|end|>"
}
```

### Validation & Testing

**Dataset Quality:**
- ‚úÖ 99,935/100,000 examples passed sqlglot validation (99.93%)
- ‚úÖ All MySQL syntax converted to PostgreSQL
- ‚úÖ No duplicate questions
- ‚úÖ Consistent ChatML formatting

**Model Quality (Checkpoint-1250):**
- ‚úÖ 30/30 real-world scenarios (100%)
- ‚úÖ 6.7/10 average on complex edge cases
- ‚úÖ Syntax correctness: 100%
- ‚úÖ Production-ready for web applications

## Troubleshooting

### Low Quality Output

**Current Status**: Quality is excellent (12.4/10) with checkpoint-500.

If retraining from scratch:
1. ‚úÖ **Use validated dataset**: gretelai/synthetic_text_to_sql with preprocessing
2. ‚úÖ **Enable Unsloth**: Set `use_unsloth: true` in manifest
3. ‚úÖ **Use recommended params**: LR 5e-5, dropout 0.1, warmup_ratio 0.1
4. ‚ö†Ô∏è **Don't overtrain**: Model converges at 25% of epoch 1

### Training OOM (Out of Memory) - Windows

**Windows-specific fixes applied**:
1. ‚úÖ **Small batch_size**: 2 (safe for Windows)
2. ‚úÖ **High gradient_accumulation**: 45 (effective batch = 90)
3. ‚úÖ **Disabled torch_compile**: Triton incompatible on Windows
4. ‚úÖ **Unsloth enabled**: 70% VRAM reduction

If still OOM:
1. Reduce `max_seq_length` from 800 to 512
2. Enable `gradient_checkpointing: true`
3. Reduce `gradient_accumulation_steps` to 30

### Slow Training

**Current**: 2x faster with Unsloth. Expected ~2-3 hours for full training.

If slower:
1. ‚úÖ **Verify Unsloth**: Check `use_unsloth: true` in manifest
2. ‚úÖ **Check CUDA**: `nvidia-smi` should show GPU usage
3. ‚úÖ **Verify SDPA**: Ensure `use_sdpa: true`
4. ‚ö†Ô∏è **Don't use torch_compile**: Disabled on Windows (Triton issue)

### Unicode Errors (Windows Console)

**Fixed**: All Unicode arrows (‚Üí) replaced with ASCII (->).

If you see encoding errors:
1. Use PowerShell instead of CMD
2. Set console to UTF-8: `[Console]::OutputEncoding = [System.Text.Encoding]::UTF8`

## Version History

### v0.2.0 (Current - Production Ready) - 2025-11-06

**Major Improvements:**
- ‚úÖ **Checkpoint-1250 selected** - Best balance of quality (9.6/10) vs generalization
- ‚úÖ **100% real-world success** - Tested on 30 practical scenarios (e-commerce, CRM, analytics)
- ‚úÖ **Enhanced packaging** - All files in root, no nested directories
- ‚úÖ **Packaging checkpoint selection** - Manifest can specify best checkpoint (not just final)
- ‚úÖ **Backward compatible loading** - Supports both old and new package structures
- ‚úÖ **Comprehensive documentation** - Known limitations and strengths clearly documented
- ‚úÖ **Production validation** - Tested with real extraction and inference

**Training Results:**
- Dataset: gretelai/synthetic_text_to_sql (99,935 examples)
- Training method: DoRA r=12 + Unsloth (2x faster, 70% less VRAM)
- Quality score: **9.6/10** on real-world benchmark
- Checkpoint evolution tested: 750 ‚Üí 1000 ‚Üí **1250 (Best)** ‚Üí 1500 (degradation)
- Windows compatible (CUDA 12.1, RTX 4090)

**Known Limitations:**
- ‚ùå Recursive CTEs (uses self-join instead)
- ‚ùå UNION operations (uses JOIN with OR)
- ‚ùå LEFT JOIN with NULL checks
- ‚ö†Ô∏è Complex percentage calculations
- ‚ö†Ô∏è Deeply nested CASE WHEN

**Package Structure Changes:**
- **Old (v0.0.1)**: `weights/adapter/adapter_model.safetensors`
- **New (v0.2.0)**: `adapter_model.safetensors` (root level)
- All 11 files now in package root for easier loading

### v0.0.1 - 2025-11-03
- ‚úÖ Initial release with checkpoint-500
- ‚úÖ Basic SQL generation capabilities
- ‚ö†Ô∏è Limited testing on real-world scenarios
- ‚ö†Ô∏è Nested directory structure in packages

### Training Stats
- **Dataset processing**: ~5 minutes (sqlglot validation)
- **Training time**: ~3-4 hours for 1.5 epochs (RTX 4090 + Unsloth)
- **Optimal checkpoint**: Checkpoint-1250 (1.25 epochs)
- **Production checkpoint**: checkpoint-1250 (best quality/generalization balance)
- **VRAM peak**: 0.56GB (training), 18MB (inference overhead)
- **Test coverage**: 30 real-world scenarios + 10 complex edge cases

## Credits

- **Base Model**: Qwen/Qwen3-0.6B
- **Dataset**: gretelai/synthetic_text_to_sql
- **Training**: Unsloth (2x speedup)
- **Validation**: sqlglot (SQL parsing)
- **Framework**: HuggingFace Transformers + PEFT + TRL

## License

CC-BY-4.0

## Tags

sql, database, text2sql, query-generation, postgres, qwen3, dora, unsloth, windows
