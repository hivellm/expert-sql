{
  "name": "expert-sql",
  "version": "0.3.0",
  "schema_version": "2.0",
  "description": "SQL query generation expert focado em respostas consistentes e curtas. Treinado em 147.140 exemplos validados de múltiplas fontes. Esta versão (0.3.0) utiliza o checkpoint-1000 — melhor equilíbrio entre qualidade e consistência, com alta taxa de acerto em queries básicas e intermediárias.",
  "author": "hivellm",
  "homepage": "https://github.com/hivellm/expert-sql",
  
  "base_models": [
    {
      "name": "F:/Node/hivellm/expert/models/Qwen3-0.6B",
      "sha256": "",
      "quantization": "int4",
      "rope_scaling": {
        "type": "ntk-by-parts",
        "factor": 8.0,
        "max_position_embeddings": 32768,
        "original_max_position_embeddings": 8192,
        "fine_grained": true,
        "_comment": "Qwen3-specific NTK-by-parts scaling (β=0.25). Matches Rust implementation (qwen3_model.rs:49-57)"
      },
      "prompt_template": "qwen3",
      "adapters": [
        {
          "type": "dora",
          "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "up_proj", "down_proj"],
          "r": 12,
          "alpha": 24,
          "scaling": "dora",
          "dropout": 0.1,
          "size_bytes": 0,
          "sha256": "",
          "_comment": "Checkpoint-1000: Melhor equilíbrio entre qualidade e consistência. Maior taxa de acerto em queries básicas e intermediárias (16/18 corretas). Selecionado após análise comparativa com checkpoints 750, 1250 e 1496."
        }
      ]
    }
  ],
  
  "soft_prompts": [],
  
  "capabilities": [
    "database:sql",
    "database:postgresql",
    "query:sql",
    "task:text2sql",
    "feature:schema_understanding",
    "feature:table_queries",
    "feature:join_queries",
    "feature:multi_table_joins",
    "feature:aggregation",
    "feature:filtering",
    "feature:subqueries",
    "feature:correlated_subqueries",
    "feature:window_functions",
    "feature:cte_non_recursive",
    "feature:date_operations",
    "feature:string_operations",
    "feature:having_clause",
    "feature:group_by",
    "feature:order_by",
    "usecase:ecommerce",
    "usecase:crm",
    "usecase:analytics",
    "usecase:reports",
    "language:en"
  ],
  
  "limitations": [
    {
      "pattern": "no_recursive_cte",
      "description": "Recursive CTEs (WITH RECURSIVE) remain unreliable - rewrites into self-joins or subqueries instead of proper recursion",
      "example": "Query: 'Find all ancestors' → Generates self-joins instead of WITH RECURSIVE",
      "workaround": "Use iterative queries or provide explicit depth limit in prompt"
    },
    {
      "pattern": "no_union_operations",
      "description": "UNION / UNION ALL generates redundant predicates or swaps to JOIN-based rewrites",
      "example": "Query: 'SELECT * FROM table1 UNION SELECT * FROM table2' → May add redundant WHERE clauses or convert to JOIN",
      "workaround": "Use explicit UNION syntax and validate generated query structure"
    },
    {
      "pattern": "limited_left_join_null",
      "description": "LEFT JOIN with NULL checks may collapse back to INNER JOIN or misapply IS NULL conditions",
      "example": "Query: 'LEFT JOIN ... WHERE right_table.id IS NULL' → May convert to INNER JOIN",
      "workaround": "Provide explicit LEFT JOIN examples in schema context or use subqueries for null filtering"
    },
    {
      "pattern": "limited_nested_case_when",
      "description": "Consistent up to two levels; deeper nesting collapses to simpler branches",
      "example": "Query with 3+ nested CASE WHEN statements → May simplify to 2 levels",
      "workaround": "Break down complex CASE WHEN into multiple queries or use simpler conditional logic"
    },
    {
      "pattern": "limited_complex_percentages",
      "description": "May inject heuristic numeric predicates on aggregate/count prompts",
      "example": "Query: 'Customers with more than 5 orders' → May add filters like 'orders.total > 50000'",
      "workaround": "Provide explicit numeric thresholds in prompt or validate generated query manually"
    }
  ],
  
  "quality_metrics": {
    "benchmark_score": 8.9,
    "test_queries": 18,
    "checkpoint": "checkpoint-1000",
    "training_steps": 1000,
    "test_date": "2025-11-12",
    "sql_valid": "16/18",
    "success_rate": 88.9,
    "_comment": "Checkpoint-1000 selected for best overall quality and consistency. Highest accuracy rate in basic and intermediate queries. Best balance between correctness and SQL quality compared to other checkpoints (750, 1250, 1496)."
  },
  
  "routing": {
    "keywords": [
      "sql",
      "database",
      "query",
      "text2sql",
      "table",
      "select",
      "join",
      "where",
      "from",
      "group by",
      "order by",
      "insert",
      "update",
      "delete",
      "users",
      "customers",
      "orders",
      "products",
      "find all",
      "find",
      "get all",
      "get",
      "show all",
      "show",
      "list all",
      "list",
      "older",
      "younger",
      "between",
      "count",
      "sum",
      "average"
    ],
    "exclude_keywords": [
      "what is",
      "what are",
      "explain",
      "meaning",
      "definition",
      "describe",
      "tell me about",
      "how does",
      "why"
    ],
    "router_hint": "database=sql OR query=sql OR task=text2sql",
    "priority": 0.85,
    "_comment": "Priority 0.85 for SQL queries. exclude_keywords prevent using expert for explanatory questions."
  },
  
  "constraints": {
    "max_chain": 10,
    "load_order": 6,
    "incompatible_with": [],
    "requires": []
  },
  
  "perf": {
    "latency_ms_overhead": 3.0,
    "vram_mb_overhead": 18,
    "supported_batch_sizes": [1, 2, 4, 8],
    "_comment": "DoRA r=12 needs 18MB VRAM (vs 15MB for LoRA). Grammar validation adds 0.5ms latency."
  },
  
  "runtime": {
    "candle_compatible": true,
    "requires_kv_cache_persistence": true,
    "attention_kernel": "flash-v2",
    "_comment": "Metadata for Rust/Candle runtime. Qwen3 uses custom flash attention kernel (not standard SDPA)."
  },
  
  "training": {
    "dataset": {
      "path": "datasets/train.jsonl",
      "format": "jsonl",
      "streaming": false,
      "source": "multi-source",
      "sources": [
        {
          "name": "gretelai/synthetic_text_to_sql",
          "original_examples": 99935,
          "processed_examples": 99935,
          "duplicates_removed": 0,
          "invalid_removed": 0
        },
        {
          "name": "Clinton/Text-to-sql-v1",
          "original_examples": 47145,
          "processed_examples": 47145,
          "duplicates_removed": 0,
          "invalid_removed": 0
        },
        {
          "name": "synthetic_fixes",
          "original_examples": 60,
          "processed_examples": 60,
          "duplicates_removed": 0,
          "invalid_removed": 0
        }
      ],
      "preprocessing": {
        "total_examples": 147140,
        "processed_examples": 147140,
        "duplicates_removed": 2855,
        "invalid_sql_removed": 0,
        "validation": "sqlglot",
        "format": "qwen3",
        "deduplication": "by_question",
        "dialect_normalization": "mysql_sqlite_to_postgresql"
      },
      "_comment": "Multi-source dataset: 147,140 validated examples (gretelai/synthetic_text_to_sql: 99,935, Clinton/Text-to-sql-v1: 47,145, synthetic_fixes: 60). Additional data sources available: bigcode/the-stack (https://huggingface.co/datasets/bigcode/the-stack/tree/main/data). Multi-dialect normalization (MySQL/SQLite->PostgreSQL), sqlglot validation, deduplicated (2,855 removed), English-only. Optimized to text-only format (77% smaller). Dataset regenerated with Qwen3 Hybrid Reasoning (75% reasoning + 25% direct outputs). Checkpoint-1250 achieved 9.6/10 quality score.",
      "field_mapping": {
        "text": "text"
      }
    },
    "config": {
      "method": "sft",
      "adapter_type": "dora",
      "use_unsloth": true,
      "_unsloth_comment": "Enable Unsloth for 2x faster training and 70% less VRAM. Requires: pip install 'unsloth[windows] @ git+https://github.com/unslothai/unsloth.git'",
      "rank": 12,
      "alpha": 24,
      "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "up_proj", "down_proj"],
      "epochs": 1.5,
      "_comment": "DoRA r=12, LLaMA-Factory optimized params. Lower LR (5e-5) + higher dropout (0.1) for better generalization.",
      "learning_rate": 5e-5,
      "_lr_comment": "Lower LR (5e-5) + higher dropout (0.1) for better generalization (LLaMA-Factory best practice).",
      "batch_size": 2,
      "_batch_comment": "Reduced from 6 to 2 for Windows memory safety. Compensated with gradient_accumulation_steps=45 (effective batch=90).",
      "gradient_accumulation_steps": 45,
      "warmup_steps": 0,
      "warmup_ratio": 0.1,
      "_warmup_comment": "Using warmup_ratio=0.1 (10%, LLaMA-Factory best practice) = ~150 steps. warmup_steps=0 when using ratio.",
      "lr_scheduler": "cosine",
      "_scheduler_comment": "Simple cosine decay. More conservative to prevent pattern overfitting.",
      "max_seq_length": 1024,
      "_seq_length_comment": "1024 tokens sufficient for SQL queries. Can be increased to 1536 for better GPU utilization if needed.",
      "dataloader_num_workers": 0,
      "dataloader_pin_memory": false,
      "dataloader_prefetch_factor": 1,
      "dataloader_persistent_workers": false,
      "_dataloader_comment": "Windows fixes: num_workers=0, pin_memory=false, persistent_workers=false. Prevents worker memory copies and leaks.",
      "fp16": false,
      "bf16": true,
      "_precision_comment": "Using bf16 as recommended by Qwen3/Unsloth documentation. Bfloat16 = TRUE is required for correct training behavior. See https://huggingface.co/unsloth/Qwen3-0.6B-GGUF",
      "use_tf32": true,
      "use_sdpa": true,
      "flash_attention_2": false,
      "packing": true,
      "memory_efficient_attention": true,
      "memory_clear_every": 100,
      "torch_compile": false,
      "torch_compile_backend": "inductor",
      "torch_compile_mode": "reduce-overhead",
      "_compile_comment": "Windows fix: torch_compile=false (Triton incompatible with PyTorch 2.5.1 on Windows). Unsloth provides enough speedup.",
      "optim": "adamw_bnb_8bit",
      "_optim_comment": "Using 8-bit optimizer for memory efficiency. Compatible with Unsloth.",
      "group_by_length": false,
      "_group_by_length_comment": "Disabled to maintain consistent batch structure. Can be enabled to reduce padding waste.",
      "logging_steps": 10,
      "save_strategy": "steps",
      "save_steps": 250,
      "save_total_limit": 4,
      "_checkpoint_strategy": "Checkpoints saved every 250 steps. Checkpoint-1000 selected for best overall quality and consistency (16/18 correct in 18 test scenarios). Best balance between correctness and SQL quality compared to other checkpoints.",
      "evaluation_strategy": "steps",
      "eval_steps": 250,
      "load_best_model_at_end": true,
      "metric_for_best_model": "eval_loss",
      "greater_is_better": false,
      "gradient_checkpointing": true,
      "_gradient_checkpointing_comment": "Enabled for memory efficiency. Reduces VRAM usage at cost of recomputation overhead.",
      "activation_checkpointing": "attention_only",
      "use_cuda_graphs": false,
      "cuda_graph_warmup_steps": 100,
      "_cuda_graphs_comment": "Windows fix: use_cuda_graphs=false (unstable on Windows). CUDA graphs require torch.compile which leaks memory."
    },
    "decoding": {
      "use_grammar": true,
      "grammar_type": "sql-postgres",
      "validation": "parser-strict",
      "stop_sequences": [";", "\n\n"],
      "temperature": 0.7,
      "top_p": 0.8,
      "top_k": 20,
      "min_p": 0,
      "_comment": "Unsloth/Qwen recommended settings: temp=0.7, top_p=0.8, top_k=20, min_p=0. Prevents repetition collapse. Grammar validation prevents syntax errors. See https://huggingface.co/unsloth/Qwen3-0.6B-GGUF"
    },
    "trained_on": "2025-11-06",
    "checkpoint": "checkpoint-1000",
    "packaging_checkpoint": "checkpoint-1000",
    "base_model_version": "qwen3-0.6b-int4",
    "alternative_checkpoints": {
      "checkpoint-1250": {
        "path": "qwen3-06b/checkpoint-1250",
        "step": 1250,
        "score": 9.6,
        "win_rate": 1.0,
        "best_for": ["general_sql_generation"],
        "_comment": "Checkpoint-1250 achieved 9.6/10 quality score but regresses to text explanations in some cases. Checkpoint-500 selected for consistent SQL output."
      }
    }
  },
  
  "license": "cc-by-4.0",
  "tags": [
    "sql",
    "database",
    "text2sql",
    "query-generation",
    "postgresql",
    "postgres",
    "ecommerce",
    "crm",
    "analytics",
    "business-intelligence",
    "dora",
    "qwen3"
  ]
}
